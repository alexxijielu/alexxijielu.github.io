<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Alex Lu</title>
    <link>http://www.alexluresearch.com/project/</link>
      <atom:link href="http://www.alexluresearch.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 03 Mar 2020 17:18:58 -0500</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Projects</title>
      <link>http://www.alexluresearch.com/project/</link>
    </image>
    
    <item>
      <title>Robust Models</title>
      <link>http://www.alexluresearch.com/project/out-of-sample/</link>
      <pubDate>Tue, 03 Mar 2020 17:18:58 -0500</pubDate>
      <guid>http://www.alexluresearch.com/project/out-of-sample/</guid>
      <description>&lt;img src=&#34;coos_preprint.png&#34;&gt;
&lt;p&gt;In addition to learning biological signal, a good machine learning model will be robust to non-biological, technical variation. Unfortunately, getting a model that learns just biological signal while ignoring technical variation can be difficult. Especially when you&amp;rsquo;re working with a purely data-driven model, both sources of variation can contribute to the predictiveness of the model. For example, large-scale image screens that parallelize many experiments at once might image experiments in the same order each time. The microscope gradually heats up as it operates, meaning the final set of experiments will be brighter than the first step of experiments, so the model might learn that image intensity can discriminate these experiments&lt;/p&gt;
&lt;p&gt;While biological signal (hopefully) produces general information, technical variation does not. If you scrambled the order of experiments in new screens, you might see a drop in performance, if the model has indeed learned to rely on technical variation as part of its decision-making. A growing concern in machine learning is that we may be overestimating how generalizable models are with standard techniques. Typically, an expert will evaluate their method by holding out some data, and then evaluating performance on that data to simulate the condition where the classifier is given fresh, unseen data. However, these types of methods don&amp;rsquo;t capture the cases where there may be changes in the way the data is produced and generated - for example, different hospitals have different instruments, so classifiers shown to work on one hospital&amp;rsquo;s data will not always work as well for another&amp;rsquo;s.&lt;/p&gt;
&lt;p&gt;To help machine learning experts develop methods that are robust to new data from different sources, &lt;a href=&#39;https://arxiv.org/abs/1906.07282&#39;&gt;we released a dataset where evaluation data increasingly differs from the data the experts are provided with to develop their models&lt;/a&gt;. For example, our hardest dataset comprises of data collected from a different laboratory in Ottawa, under a different microscope. We found that all of the current methods used to classify cells are not robust when the new data is different enough from the original data; we hope that by providing this dataset, machine learning experts will eventually be able to build methods that are robust to these effects.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
